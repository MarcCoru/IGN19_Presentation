
\begin{frame}
	\frametitle{Area of Interest in Bavaria}
	\includegraphics[width=\textwidth]{images/aoi}
\end{frame}



\begin{frame}[t]
\frametitle{Looking at Sequence to Sequence Models from NLP}
%	\framesubtitle{Natürlicher Sprachverarbeitung, Übersetzung, Spracherkennung}

%Verbreitet in sequenziellen Aufgabengebieten, wie natürlicher Sprachverarbeitung, Übersetzung, Spracherkennung
%	\begin{center}
%		\large Wort-Sequenz $\rightarrow$ Representation $c$ $\rightarrow$ Wort-Sequenz
%	\end{center}
\begin{center}
	\input{images/seq2seq.tikz}
\end{center}
%\begin{columns}
%	\column{.4\textwidth}
%	%		\brand{Word2Vec} embedding 
%	%		Neural Machine Translation encodes 
%	
%	\centering 
%%	
%%	\begin{tikzpicture}
%%	\node[fill=tumbluelight!50,rounded corners](x){Eingabesequenz};
%%	\node[fill=tumbluelight!50,rounded corners, below=of x](c){Repräsentation};
%%	\node[fill=tumbluelight!50,rounded corners, below=of c](o){Ausgabsequenz};
%%	
%%	\draw[-stealth] (x) -- (c);
%%	\draw[-stealth] (c) -- (o);
%%	
%%	\end{tikzpicture}
%%	
%	\column{.6\textwidth}
%	
%\end{columns}


{\small
	Sutskever, I., Vinyals, O., \& Le, Q. V. (2014). Sequence to sequence learning with neural networks. In Advances in neural information processing systems (pp. 3104-3112).}


\end{frame}

\input{images/encoder.tikz}
\begin{frame}
\frametitle{Temporal Vegetation Modelling with LSTMs}
\framesubtitle{CVPR Earthvision 2017}
\begin{columns}
	\column{.5\textwidth}
	\figencoderfieldRNN
	\column{.5\textwidth}
	\includegraphics[width=.8\textwidth]{images/confmat_fieldrnn}
%		images/stmelfadditional/Genauigkeiten/accuracies1}
\end{columns}

\vspace{1em}

\textsl{\small
	Rußwurm, M. and Körner, M. (2017). \textbf{Temporal Vegetation Modelling using Long Short-Term Memory Networks for Crop Identification from Medium-Resolution Multi-Spectral Satellite Images}. In IEEE/ISPRS EarthVision 2017 Workshop, Proceedings of the IEEE CVPR Workshops. \textbf{\color{tumorange} Best Paper Award}
}

\end{frame}



\def\fps{3}
\input{images/cells.tikz}

\begin{frame}[t]
	\frametitle{Convolutional Long-Short Term Memory Cells (ConvLSTM)}
	
	%	Reveal Sequence:
	%	1: update, empty cell
	%	2: forget gate
	%	3: input gate + modulation gate + jmult
	%	4: output gate
	%	5: cell state
	%	6: output
	
	\begin{columns}
		\begin{column}{0.3\textwidth}
			\begin{tabularx}{\textwidth}{l}
				LSTM update: \\
				$\VHidden_t,\VCellState_t  \leftarrow \VInput_t, \VHidden_{t-1}, \VCellState_{t-1}$\\
				\\
				\visible<2->{Internal gates:} \\
				\visible<2->{$\VForgetGate_t = \sigma( \conv{\concat{ \VInput_t }{ \VHidden_{t-1} } }{ \MWeight_f } + 1 )$} \\
				\visible<3->{$\VInputGate_t = \sigma( \conv{ \concat{ \VInput_t }{ \VHidden_{t-1} } }{ \MWeight_i } ) $} \\
				\visible<3->{$\VModulationGate_t = \sigma( \conv{ \concat{ \VInput_t }{ \VHidden_{t-1} } }{ \MWeight_j } ) $} \\
				\visible<4->{$\VOutputGate_t = \sigma( \conv{ \concat{ \VInput_t }{ \VHidden_{t-1} } }{ \MWeight_o} ) $} \\
				\\
				\visible<5->{Internal cell state:} \\
				\visible<5->{$\VCellState_t = \VCellState_{t-1} \odot \VForgetGate_t + \VInputGate_t \odot \VModulationGate_t$} \\
				\visible<6->{Output:} \\
				\visible<6->{$\VHidden_t= \VOutputGate_t \odot \tanh(\VCellState_t) $} \\
			\end{tabularx}
		\end{column}
		\begin{column}{0.7\textwidth}
			\lstmexplain
		\end{column}
	\end{columns}
\end{frame}


%\input{images/encoder.tikz}


%\begin{frame}
%\frametitle{Classification ConvLSTM Network}
%\input{images/network.tikz}
%\end{frame}


\begin{frame}[t]
\frametitle{Recurrent Convolutional Neural Networks}
\framesubtitle{Convolutional Long Short-Term Memory --- ConvLSTM (Hochreiter \& Schmidhuber, 1997)}
\begin{columns}
	\begin{column}{0.3\textwidth}
		
		
		\textbf{Convolutional LSTM:}
		
		\vspace{2em}
		{\scriptsize
			Xingjian, S. H. I., Chen, Z., Wang, H., Yeung, D. Y., Wong, W. K., \& Woo, W. C. (2015). Convolutional LSTM network: A machine learning approach for precipitation nowcasting. In Advances in neural information processing systems (pp. 802-810). \par
		}
		
	\end{column}
	\vspace{-2em}	
	\begin{column}{0.7\textwidth}
		
		\includegraphics[width=\textwidth]{images/convlstm}	
		%\lstmexplain
	\end{column}
\end{columns}



\end{frame}

\begin{frame}
\frametitle{Classification ConvLSTM Network}

\centering\includegraphics[width=.8\textwidth]{images/lstm}

	\small
\textsl{
	Rußwurm M., Körner M. (2018). \textbf{Multi-Temporal Land Cover Classification with Sequential Recurrent Encoders}. In ISPRS International Journal of Geo-Information. 
}

\end{frame}


%{
%	\setbeamercolor{background canvas}{bg=tumbluedark}

\begin{frame}

\vfill
\Huge\color{black}
\begin{center}
	\begin{columns}
		\column{\textwidth}
		
		
		\includegraphics[width=\textwidth]{images/x_1}
		\includegraphics[width=\textwidth]{images/x_2}
		\includegraphics[width=\textwidth]{images/x_3}
		
		\vspace{3em}
		
		
		\textbf{\color{tumbluedark}Surprise:} 
		\hfill It worked without specifically labeling clouds!
%		\only<1>{Surprise:} \only<2>{It worked without labeling clouds!}
		%			\includegraphics[width=5cm]{images/dscovrepic/epic1}
		%\includegraphics[width=7cm]{images/fdl}
	\end{columns}
\end{center}

\vfill

\end{frame}
%}

\begin{frame}
\frametitle{ConvLSTM robust to clouds}
\input{images/scl.tikz}
\input{images/clouds.tikz}
\end{frame}


\begin{frame}
	\frametitle{Remembering Karpathy's "Unreasonable Effectiveness of RNNs"}
	
	\includegraphics[width=\textwidth]{images/karpathy}
	
	\url{http://karpathy.github.io/2015/05/21/rnn-effectiveness/}
\end{frame}

\input{images/activations.tikz}
\begin{frame}
\frametitle{Internal States Encode increasingly Classification Features}
\framesubtitle{LSTM cell \textbf{47} of 256}
\figactivations{1}{3}
\end{frame}
%

\begin{frame}
\frametitle{Found Cloud Masking Cells in the RNN}
\framesubtitle{LSTM cell \textbf{47} of 256}
\figactivations{1}{22}
\end{frame}

\begin{frame}
\frametitle{Found Cloud Masking Cells in the RNN}
\framesubtitle{LSTM cell \textbf{47} of 256}
\figactivations{1}{47}
\end{frame}
%
%\begin{frame}
%	\frametitle{Publication and Github}
%	
%			Including the spatial information by adding convolutions
%	\vspace{2em}
%	{\small
%		Rußwurm, M., \& Körner, M. (2018). Multi-temporal land cover classification with sequential recurrent encoders. ISPRS International Journal of Geo-Information, 7(4), 129.
%	}
%
%	Github
%	\url{https://github.com/tum-lmf/mtlcc}
%	
%\end{frame}


\begin{frame}[c]
\frametitle{Paper and Code}
\centering 

%	\vspace{3em}

\large



Github + DockerHub + Continuation with GAF AG

\vspace{1ex}

\includegraphics[width=2cm]{images/github} \hspace{.5ex}
\includegraphics[width=2cm]{images/qr_github} \hspace{.5ex}
\includegraphics[width=2cm]{images/docker} \hspace{.5ex}
\vline
\hspace{.5ex}
\includegraphics[width=4cm]{images/gaf}

\vspace{1ex}

\url{https://github.com/TUM-LMF/MTLCC}
\url{https://github.com/TUM-LMF/MTLCC-pytorch}

\url{http://www.lmf.bgu.tum.de/vision/}

\vspace{1em}
\begin{columns}[t]
	\column{.5\textwidth}
	\scriptsize
	\textsl{
		Rußwurm, M. and Körner, M. (2017). \textbf{Temporal Vegetation Modelling using Long Short-Term Memory Networks for Crop Identification from Medium-Resolution Multi-Spectral Satellite Images}. In IEEE/ISPRS EarthVision 2017 Workshop, Proceedings of the IEEE CVPR Workshops.
	}
	
	\column{.5\textwidth}
	\small
	\textsl{
		Rußwurm M., Körner M. (2018). \textbf{Multi-Temporal Land Cover Classification with Sequential Recurrent Encoders}. ISPRS International Journal of Geo-Information. https://arxiv.org/abs/1802.02080. (in review)
	}
	
\end{columns}


\end{frame}

%\begin{frame}
%\frametitle{Found Cloud Masking Cells in the RNN}
%\framesubtitle{LSTM cell \textbf{47} of 256}
%\figactivations{1}{3}
%\end{frame}
